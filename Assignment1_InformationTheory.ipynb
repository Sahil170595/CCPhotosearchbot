{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ4LOP2IBH9Nj4nDp+VrFF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahil170595/CCPhotosearchbot/blob/main/Assignment1_InformationTheory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Information Theory Basics"
      ],
      "metadata": {
        "id": "Uht-32LKJWqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Information Theory and Mutual Information Calculation\n",
        "\n",
        "Claude Shannon introduced Information Theory in 1948, which plays a crucial role in data transmission, cryptography, and AI. Given the joint probability distribution:\n",
        "\n",
        "\\\n",
        "\\begin{array}{c|cccc}\n",
        "Y \\backslash X & 1 & 2 & 3 & 4 \\\\\n",
        "\\hline\n",
        "1 & \\frac{1}{8} & \\frac{1}{16} & \\frac{1}{32} & \\frac{1}{32} \\\\\n",
        "2 & \\frac{1}{16} & \\frac{1}{8} & \\frac{1}{32} & \\frac{1}{32} \\\\\n",
        "3 & \\frac{1}{16} & \\frac{1}{16} & \\frac{1}{16} & \\frac{1}{16} \\\\\n",
        "4 & \\frac{1}{4} & 0 & 0 & 0 \\\\\n",
        "\\end{array}\n",
        "\n",
        "\n",
        "We will answer the following:\n",
        "\n",
        "## 1. Check if \\( H(X|Y) = H(Y|X) \\)\n",
        "\n",
        "### Compute Marginal Distributions\n",
        "\n",
        "The marginal probabilities \\( P(X) \\) and \\( P(Y) \\) are calculated by summing over respective rows and columns:\n",
        "$$\n",
        "\\\n",
        "P(X=1) = \\frac{1}{8} + \\frac{1}{16} + \\frac{1}{16} + \\frac{1}{4} = \\frac{7}{16}\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "P(X=2) = \\frac{1}{16} + \\frac{1}{8} + \\frac{1}{16} + 0 = \\frac{5}{16}\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "P(X=3) = \\frac{1}{32} + \\frac{1}{32} + \\frac{1}{16} + 0 = \\frac{2}{16} = \\frac{1}{8}\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "P(X=4) = \\frac{1}{32} + \\frac{1}{32} + \\frac{1}{16} + 0 = \\frac{2}{16} = \\frac{1}{8}\n",
        "\\\n",
        "$$\n",
        "Similarly, summing down the columns:\n",
        "$$\n",
        "\\\n",
        "P(Y=1) = \\frac{1}{8} + \\frac{1}{16} + \\frac{1}{32} + \\frac{1}{32} = \\frac{7}{32}\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "P(Y=2) = \\frac{1}{16} + \\frac{1}{8} + \\frac{1}{32} + \\frac{1}{32} = \\frac{7}{32}\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "P(Y=3) = \\frac{1}{16} + \\frac{1}{16} + \\frac{1}{16} + \\frac{1}{16} = \\frac{4}{16} = \\frac{1}{4}\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "P(Y=4) = \\frac{1}{4} + 0 + 0 + 0 = \\frac{1}{4}\n",
        "\\\n",
        "$$\n",
        "### Compute Conditional Entropies\n",
        "\n",
        "The conditional entropy is:\n",
        "$$\n",
        "\\\n",
        "H(X|Y) = \\sum_{y} P(y) H(X|Y=y)\n",
        "\\\n",
        "\n",
        "and similarly,\n",
        "$$\n",
        "\\\n",
        "H(Y|X) = \\sum_{x} P(x) H(Y|X=x)\n",
        "\\\n",
        "$$\n",
        "After detailed computation, if $$\\ H(X|Y) \\neq H(Y|X) \\ $$, then they are not equal.\n",
        "\n",
        "## 2. Verify \\( H(X) - H(X|Y) = H(Y) - H(Y|X) \\)\n",
        "\n",
        "Using the entropy formula:\n",
        "$$\n",
        "\\\n",
        "H(X) = -\\sum_{x} P(x) \\log P(x)\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "H(Y) = -\\sum_{y} P(y) \\log P(y)\n",
        "\\\n",
        "$$\n",
        "$$\n",
        "\\\n",
        "I(X, Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)\n",
        "\\\n",
        "$$\n",
        "Mutual information should be computed to verify this.\n",
        "\n",
        "## 3. Compute Mutual Information \\( I(X, Y) \\)\n",
        "$$\n",
        "\\\n",
        "I(X, Y) = \\sum_{x,y} P(x,y) \\log \\frac{P(x,y)}{P(x)P(y)}\n",
        "\\\n",
        "$$\n",
        "\n",
        "Plugging in the values from the joint and marginal distributions, we compute:\n",
        "$$\n",
        "\\\n",
        "I(X, Y) = H(X) + H(Y) - H(X, Y)\n",
        "\\\n",
        "$$\n",
        "After calculations, we obtain the numerical value of \\( I(X, Y) \\).\n",
        "\n",
        "Thus, we complete the mutual information computation.\n"
      ],
      "metadata": {
        "id": "mU6EKuwTjnEN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MiSsPozyDGnj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sympy import symbols, log, simplify"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the joint probability distribution P(X,Y)\n",
        "P_xy = {\n",
        "    (1, 1): 1/8, (1, 2): 1/16, (1, 3): 1/32, (1, 4): 1/32,\n",
        "    (2, 1): 1/16, (2, 2): 1/8, (2, 3): 1/32, (2, 4): 1/32,\n",
        "    (3, 1): 1/16, (3, 2): 1/16, (3, 3): 1/16, (3, 4): 1/16,\n",
        "    (4, 1): 1/4,  (4, 2): 0,    (4, 3): 0,    (4, 4): 0\n",
        "}\n",
        "\n",
        "# [Probability and entropy calculations]\n",
        "P_x = {x: sum(P_xy.get((x, y), 0) for y in range(1, 5)) for x in range(1, 5)}\n",
        "P_y = {y: sum(P_xy.get((x, y), 0) for x in range(1, 5)) for y in range(1, 5)}\n",
        "\n",
        "def entropy(P):\n",
        "    return -sum(p * log(p, 2) for p in P.values() if p > 0)\n",
        "\n",
        "H_x = entropy(P_x)\n",
        "H_y = entropy(P_y)\n",
        "H_xy = entropy(P_xy)\n",
        "H_x_given_y = H_xy - H_y\n",
        "H_y_given_x = H_xy - H_x\n",
        "I_xy = H_x - H_x_given_y"
      ],
      "metadata": {
        "id": "7Oc7dwQ3Ggvd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nInformation Theory Metrics Analysis:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(f\"\\nH(X) = {H_x.evalf():.4f} bits\")\n",
        "print(\"Marginal Entropy of X:\")\n",
        "print(f\"- Measures uncertainty in X alone\")\n",
        "print(f\"- Maximum value would be log2({len(P_x)}) = {log(len(P_x), 2).evalf():.4f} bits for uniform distribution\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLhKe9KDGUe4",
        "outputId": "d5801eb9-ef65-49d4-c7ce-d306b347adca"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Information Theory Metrics Analysis:\n",
            "--------------------------------------------------\n",
            "\n",
            "H(X) = 2.0000 bits\n",
            "Marginal Entropy of X:\n",
            "- Measures uncertainty in X alone\n",
            "- Maximum value would be log2(4) = 2.0000 bits for uniform distribution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nH(Y) = {H_y.evalf():.4f} bits\")\n",
        "print(\"Marginal Entropy of Y:\")\n",
        "print(f\"- Measures uncertainty in Y alone\")\n",
        "print(f\"- Maximum value would be log2({len(P_y)}) = {log(len(P_y), 2).evalf():.4f} bits for uniform distribution\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Rg5Su3gGN7m",
        "outputId": "5542d9f6-e050-45af-ce5c-a7e457bd5247"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "H(Y) = 1.7500 bits\n",
            "Marginal Entropy of Y:\n",
            "- Measures uncertainty in Y alone\n",
            "- Maximum value would be log2(4) = 2.0000 bits for uniform distribution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nH(X,Y) = {H_xy.evalf():.4f} bits\")\n",
        "print(\"Joint Entropy:\")\n",
        "print(\"- Measures total uncertainty in the joint distribution\")\n",
        "print(f\"- Maximum possible value: log2({len(P_x)}*{len(P_y)}) = {log(len(P_x)*len(P_y), 2).evalf():.4f} bits\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AWPu4VXGK73",
        "outputId": "23eec44c-686e-44ec-9f6f-d8a202a4041c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "H(X,Y) = 3.3750 bits\n",
            "Joint Entropy:\n",
            "- Measures total uncertainty in the joint distribution\n",
            "- Maximum possible value: log2(4*4) = 4.0000 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nH(X|Y) = {H_x_given_y.evalf():.4f} bits\")\n",
        "print(\"Conditional Entropy of X given Y:\")\n",
        "print(\"- Average uncertainty remaining about X after observing Y\")\n",
        "print(f\"- Relationship: H(X|Y) = H(X,Y) - H(Y) = {H_xy.evalf():.4f} - {H_y.evalf():.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWjt2OXGGC5j",
        "outputId": "d112148a-94d8-495e-b50d-6b6565f0494c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "H(X|Y) = 1.6250 bits\n",
            "Conditional Entropy of X given Y:\n",
            "- Average uncertainty remaining about X after observing Y\n",
            "- Relationship: H(X|Y) = H(X,Y) - H(Y) = 3.3750 - 1.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nH(Y|X) = {H_y_given_x.evalf():.4f} bits\")\n",
        "print(\"Conditional Entropy of Y given X:\")\n",
        "print(\"- Average uncertainty remaining about Y after observing X\")\n",
        "print(f\"- Relationship: H(Y|X) = H(X,Y) - H(X) = {H_xy.evalf():.4f} - {H_x.evalf():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mk29hr6GAzh",
        "outputId": "7eb3b3b0-18d7-4f59-b4b5-69b42e0df99e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "H(Y|X) = 1.3750 bits\n",
            "Conditional Entropy of Y given X:\n",
            "- Average uncertainty remaining about Y after observing X\n",
            "- Relationship: H(Y|X) = H(X,Y) - H(X) = 3.3750 - 2.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nI(X;Y) = {I_xy.evalf():.4f} bits\")\n",
        "print(\"Mutual Information:\")\n",
        "print(\"- Measures mutual dependence between X and Y\")\n",
        "print(\"- Equivalent to: H(X) - H(X|Y) = H(Y) - H(Y|X)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS-gvoFOF-O4",
        "outputId": "6d607c5c-6671-4fd2-b896-1e6944bb54fd"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "I(X;Y) = 0.3750 bits\n",
            "Mutual Information:\n",
            "- Measures mutual dependence between X and Y\n",
            "- Equivalent to: H(X) - H(X|Y) = H(Y) - H(Y|X)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nMathematical Proofs and Calculations:\")\n",
        "print(\"=\" * 50)\n",
        "# Print probability table\n",
        "print(\"Joint Probability Table:\\n\")\n",
        "print(\" Y \\ X | 1    | 2    | 3    | 4    \")\n",
        "print(\"----------------------------------\")\n",
        "for y in range(1, 5):\n",
        "    row = [P_xy.get((x, y), 0) for x in range(1, 5)]\n",
        "    print(f\"  {y}   | \" + \" | \".join(f\"{p:.3f}\" for p in row))\n",
        "\n",
        "# a. Verify if H(x|y) = H(y|x)\n",
        "print(\"\\na. Testing if H(x|y) = H(y|x):\")\n",
        "print(f\"H(x|y) = {H_x_given_y.evalf():.6f}\")\n",
        "print(f\"H(y|x) = {H_y_given_x.evalf():.6f}\")\n",
        "print(f\"Difference = {abs(H_x_given_y - H_y_given_x).evalf():.6f}\")\n",
        "print(f\"Are they equal? {abs(H_x_given_y - H_y_given_x) < 1e-10}\")\n",
        "\n",
        "# b. Verify if H(x) - H(x|y) = H(y) - H(y|x)\n",
        "print(\"\\nb. Testing if H(x) - H(x|y) = H(y) - H(y|x):\")\n",
        "left_side = H_x - H_x_given_y\n",
        "right_side = H_y - H_y_given_x\n",
        "print(f\"Left side [H(x) - H(x|y)] = {left_side.evalf():.6f}\")\n",
        "print(f\"Right side [H(y) - H(y|x)] = {right_side.evalf():.6f}\")\n",
        "print(f\"Difference = {abs(left_side - right_side).evalf():.6f}\")\n",
        "print(f\"Are they equal? {abs(left_side - right_side) < 1e-10}\")\n",
        "\n",
        "# c. Calculate mutual information I(x,y)\n",
        "print(\"\\nc. Mutual Information I(x,y):\")\n",
        "print(f\"I(x,y) = H(x) - H(x|y) = {I_xy.evalf():.6f}\")\n",
        "print(f\"        = H(y) - H(y|x) = {(H_y - H_y_given_x).evalf():.6f}\")\n",
        "print(\"\\nVerification through different formulas:\")\n",
        "print(f\"1. I(x,y) = H(x) + H(y) - H(x,y) = {(H_x + H_y - H_xy).evalf():.6f}\")\n",
        "print(f\"2. I(x,y) = H(x) - H(x|y) = {(H_x - H_x_given_y).evalf():.6f}\")\n",
        "print(f\"3. I(x,y) = H(y) - H(y|x) = {(H_y - H_y_given_x).evalf():.6f}\")\n",
        "\n",
        "# Summary\n",
        "print(\"\\nSummary of Results:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"1. H(x|y) ≠ H(y|x) [They are NOT equal]\")\n",
        "print(f\"2. H(x) - H(x|y) = H(y) - H(y|x) = I(x,y) [This equality holds]\")\n",
        "print(f\"3. Mutual Information I(x,y) = {I_xy.evalf():.6f} bits\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYg1O5joHG9F",
        "outputId": "b9df4db0-8426-44b1-954b-bc3bce59d553"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mathematical Proofs and Calculations:\n",
            "==================================================\n",
            "Joint Probability Table:\n",
            "\n",
            " Y \\ X | 1    | 2    | 3    | 4    \n",
            "----------------------------------\n",
            "  1   | 0.125 | 0.062 | 0.062 | 0.250\n",
            "  2   | 0.062 | 0.125 | 0.062 | 0.000\n",
            "  3   | 0.031 | 0.031 | 0.062 | 0.000\n",
            "  4   | 0.031 | 0.031 | 0.062 | 0.000\n",
            "\n",
            "a. Testing if H(x|y) = H(y|x):\n",
            "H(x|y) = 1.625000\n",
            "H(y|x) = 1.375000\n",
            "Difference = 0.250000\n",
            "Are they equal? False\n",
            "\n",
            "b. Testing if H(x) - H(x|y) = H(y) - H(y|x):\n",
            "Left side [H(x) - H(x|y)] = 0.375000\n",
            "Right side [H(y) - H(y|x)] = 0.375000\n",
            "Difference = 0.000000\n",
            "Are they equal? True\n",
            "\n",
            "c. Mutual Information I(x,y):\n",
            "I(x,y) = H(x) - H(x|y) = 0.375000\n",
            "        = H(y) - H(y|x) = 0.375000\n",
            "\n",
            "Verification through different formulas:\n",
            "1. I(x,y) = H(x) + H(y) - H(x,y) = 0.375000\n",
            "2. I(x,y) = H(x) - H(x|y) = 0.375000\n",
            "3. I(x,y) = H(y) - H(y|x) = 0.375000\n",
            "\n",
            "Summary of Results:\n",
            "==================================================\n",
            "1. H(x|y) ≠ H(y|x) [They are NOT equal]\n",
            "2. H(x) - H(x|y) = H(y) - H(y|x) = I(x,y) [This equality holds]\n",
            "3. Mutual Information I(x,y) = 0.375000 bits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6FP3pgKVIE3F"
      },
      "execution_count": 34,
      "outputs": []
    }
  ]
}